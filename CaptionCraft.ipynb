{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2oGJMff7nrJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append('/opt/cocoapi/PythonAPI')\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "# initialize COCO API for instance annotations\n",
        "dataDir = '/opt/cocoapi'\n",
        "dataType = 'val2014'\n",
        "instances_annFile = os.path.join(dataDir, 'annotations/instances_{}.json'.format(dataType))\n",
        "coco = COCO(instances_annFile)\n",
        "\n",
        "# initialize COCO API for caption annotations\n",
        "captions_annFile = os.path.join(dataDir, 'annotations/captions_{}.json'.format(dataType))\n",
        "coco_caps = COCO(captions_annFile)\n",
        "\n",
        "# get image ids\n",
        "ids = list(coco.anns.keys())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(coco.anns.values())[0]"
      ],
      "metadata": {
        "id": "KHwhM5mcWCTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# pick a random image and obtain the corresponding URL\n",
        "ann_id = np.random.choice(ids)\n",
        "img_id = coco.anns[ann_id]['image_id']\n",
        "img = coco.loadImgs(img_id)[0]\n",
        "url = img['coco_url']\n",
        "\n",
        "# print URL and visualize corresponding image\n",
        "print(url)\n",
        "I = io.imread(url)\n",
        "plt.axis('off')\n",
        "plt.imshow(I)\n",
        "plt.show()\n",
        "\n",
        "# load and display captions\n",
        "annIds = coco_caps.getAnnIds(imgIds=img['id']);\n",
        "anns = coco_caps.loadAnns(annIds)\n",
        "coco_caps.showAnns(anns)"
      ],
      "metadata": {
        "id": "-OLaRh7mWCQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/opt/cocoapi/PythonAPI')\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from data_loader import get_loader\n",
        "from torchvision import transforms\n",
        "from pycocotools.coco import COCO"
      ],
      "metadata": {
        "id": "0e_ppsruWCNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a transform to pre-process the training images.\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
        "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
        "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
        "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
        "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
        "                         (0.229, 0.224, 0.225))])\n",
        "\n",
        "# Set the minimum word count threshold.\n",
        "vocab_threshold = 5\n",
        "\n",
        "# Specify the batch size.\n",
        "batch_size = 10\n",
        "\n",
        "# Obtain the data loader.\n",
        "data_loader = get_loader(transform=transform_train,\n",
        "                         mode='train',\n",
        "                         batch_size=batch_size,\n",
        "                         vocab_threshold=vocab_threshold,\n",
        "                         vocab_from_file=False)"
      ],
      "metadata": {
        "id": "wlc-G3A1WCJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_caption = 'A person doing a trick on a rail while riding a skateboard.'"
      ],
      "metadata": {
        "id": "8UjPbZDnWCGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "sample_tokens = nltk.tokenize.word_tokenize(str(sample_caption).lower())\n",
        "print(sample_tokens)"
      ],
      "metadata": {
        "id": "-aSUisbvWCC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_caption = []\n",
        "\n",
        "start_word = data_loader.dataset.vocab.start_word\n",
        "print('Special start word:', start_word)\n",
        "sample_caption.append(data_loader.dataset.vocab(start_word))\n",
        "print(sample_caption)"
      ],
      "metadata": {
        "id": "6aAg-C5wWB_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_caption.extend([data_loader.dataset.vocab(token) for token in sample_tokens])\n",
        "print(sample_caption)"
      ],
      "metadata": {
        "id": "IVV_wVqdWB7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "end_word = data_loader.dataset.vocab.end_word\n",
        "print('Special end word:', end_word)\n",
        "\n",
        "sample_caption.append(data_loader.dataset.vocab(end_word))\n",
        "print(sample_caption)"
      ],
      "metadata": {
        "id": "J9k7JNorWB41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "sample_caption = torch.Tensor(sample_caption).long()\n",
        "print(sample_caption)"
      ],
      "metadata": {
        "id": "Z-5f7QIyWB1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview the word2idx dictionary.\n",
        "dict(list(data_loader.dataset.vocab.word2idx.items())[:10])"
      ],
      "metadata": {
        "id": "_Baz62L6WByC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the total number of keys in the word2idx dictionary.\n",
        "print('Total number of tokens in vocabulary:', len(data_loader.dataset.vocab))"
      ],
      "metadata": {
        "id": "RXXu6T_UWBt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify the minimum word count threshold.\n",
        "vocab_threshold = 4\n",
        "\n",
        "# Obtain the data loader.\n",
        "data_loader = get_loader(transform=transform_train,\n",
        "                         mode='train',\n",
        "                         batch_size=batch_size,\n",
        "                         vocab_threshold=vocab_threshold,\n",
        "                         vocab_from_file=False)"
      ],
      "metadata": {
        "id": "Rf39BfjPWBqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the total number of keys in the word2idx dictionary.\n",
        "print('Total number of tokens in vocabulary:', len(data_loader.dataset.vocab))"
      ],
      "metadata": {
        "id": "aRKSKiMQWBnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unk_word = data_loader.dataset.vocab.unk_word\n",
        "print('Special unknown word:', unk_word)\n",
        "\n",
        "print('All unknown words are mapped to this integer:', data_loader.dataset.vocab(unk_word))"
      ],
      "metadata": {
        "id": "0dSqKUx7WBji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_loader.dataset.vocab('jfkafejw'))\n",
        "print(data_loader.dataset.vocab('ieowoqjf'))"
      ],
      "metadata": {
        "id": "JH1SK6ucWBgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the data loader (from file). Note that it runs much faster than before!\n",
        "data_loader = get_loader(transform=transform_train,\n",
        "                         mode='train',\n",
        "                         batch_size=batch_size,\n",
        "                         vocab_from_file=True)"
      ],
      "metadata": {
        "id": "PO0EauDkWBcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Tally the total number of training captions with each length.\n",
        "counter = Counter(data_loader.dataset.caption_lengths)\n",
        "lengths = sorted(counter.items(), key=lambda pair: pair[1], reverse=True)\n",
        "for value, count in lengths:\n",
        "    print('value: %2d --- count: %5d' % (value, count))"
      ],
      "metadata": {
        "id": "RWZHDz-GWBY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch.utils.data as data\n",
        "\n",
        "# Randomly sample a caption length, and sample indices with that length.\n",
        "indices = data_loader.dataset.get_train_indices()\n",
        "print('sampled indices:', indices)\n",
        "\n",
        "# Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
        "new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
        "data_loader.batch_sampler.sampler = new_sampler\n",
        "\n",
        "# Obtain the batch.\n",
        "images, captions = next(iter(data_loader))\n",
        "\n",
        "print('images.shape:', images.shape)\n",
        "print('captions.shape:', captions.shape)\n",
        "\n",
        "# (Optional) Uncomment the lines of code below to print the pre-processed images and captions.\n",
        "# print('images:', images)\n",
        "# print('captions:', captions)"
      ],
      "metadata": {
        "id": "Dai-PWqcWBVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Watch for any changes in model.py, and re-load it automatically.\n",
        "% load_ext autoreload\n",
        "% autoreload 2\n",
        "\n",
        "# Import EncoderCNN and DecoderRNN.\n",
        "from model import EncoderCNN, DecoderRNN"
      ],
      "metadata": {
        "id": "hqJGzalsWBSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "C6LRcmHBWBOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "OcXJGtTKWBLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[0].shape"
      ],
      "metadata": {
        "id": "kdVQRAgJWBH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the dimensionality of the image embedding.\n",
        "embed_size = 256\n",
        "\n",
        "#-#-#-# Do NOT modify the code below this line. #-#-#-#\n",
        "\n",
        "# Initialize the encoder. (Optional: Add additional arguments if necessary.)\n",
        "encoder = EncoderCNN(embed_size)\n",
        "\n",
        "# Move the encoder to GPU if CUDA is available.\n",
        "encoder.to(device)\n",
        "\n",
        "# Move last batch of images (from Step 2) to GPU if CUDA is available.\n",
        "images = images.to(device)\n",
        "\n",
        "# Pass the images through the encoder.\n",
        "features = encoder(images)\n",
        "\n",
        "print('type(features):', type(features))\n",
        "print('features.shape:', features.shape)\n",
        "\n",
        "# Check that your encoder satisfies some requirements of the project! :D\n",
        "assert type(features)==torch.Tensor, \"Encoder output needs to be a PyTorch Tensor.\"\n",
        "assert (features.shape[0]==batch_size) & (features.shape[1]==embed_size), \"The shape of the encoder output is incorrect.\""
      ],
      "metadata": {
        "id": "pY6sRVl7WBEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features[0].shape"
      ],
      "metadata": {
        "id": "Ad5D7CImWBA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from model import EncoderCNN, DecoderRNN"
      ],
      "metadata": {
        "id": "TQ6pyDFGWA9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the number of features in the hidden state of the RNN decoder.\n",
        "hidden_size = 512\n",
        "\n",
        "#-#-#-# Do NOT modify the code below this line. #-#-#-#\n",
        "\n",
        "# Store the size of the vocabulary.\n",
        "vocab_size = len(data_loader.dataset.vocab)\n",
        "\n",
        "# Initialize the decoder.\n",
        "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
        "\n",
        "# Move the decoder to GPU if CUDA is available.\n",
        "decoder.to(device)\n",
        "\n",
        "# Move last batch of captions (from Step 1) to GPU if CUDA is available\n",
        "captions = captions.to(device)\n",
        "\n",
        "# Pass the encoder output and captions through the decoder.\n",
        "outputs = decoder(features, captions)\n",
        "\n",
        "print('type(outputs):', type(outputs))\n",
        "print('outputs.shape:', outputs.shape)\n",
        "\n",
        "# Check that your decoder satisfies some requirements of the project! :D\n",
        "assert type(outputs)==torch.Tensor, \"Decoder output needs to be a PyTorch Tensor.\"\n",
        "assert (outputs.shape[0]==batch_size) & (outputs.shape[1]==captions.shape[1]) & (outputs.shape[2]==vocab_size), \"The shape of the decoder output is incorrect.\""
      ],
      "metadata": {
        "id": "W7Ifi2z8WA5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import sys\n",
        "sys.path.append('/opt/cocoapi/PythonAPI')\n",
        "from pycocotools.coco import COCO\n",
        "from data_loader import get_loader\n",
        "from model import EncoderCNN, DecoderRNN\n",
        "import math\n",
        "\n",
        "\n",
        "## Select appropriate values for the Python variables below.\n",
        "batch_size = 10          # batch size\n",
        "vocab_threshold = 4        # minimum word count threshold\n",
        "vocab_from_file = True      # if True, load existing vocab file\n",
        "embed_size = 256           # dimensionality of image and word embeddings\n",
        "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
        "num_epochs = 3             # number of training epochs\n",
        "save_every = 1             # determines frequency of saving model weights\n",
        "print_every = 100          # determines window for printing average loss\n",
        "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
        "\n",
        "# (Optional)  Amend the image transform below.\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
        "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
        "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
        "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
        "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
        "                         (0.229, 0.224, 0.225))])\n",
        "\n",
        "# Build data loader.\n",
        "data_loader = get_loader(transform=transform_train,\n",
        "                         mode='train',\n",
        "                         batch_size=batch_size,\n",
        "                         vocab_threshold=vocab_threshold,\n",
        "                         vocab_from_file=vocab_from_file)\n",
        "\n",
        "# The size of the vocabulary.\n",
        "vocab_size = len(data_loader.dataset.vocab)\n",
        "\n",
        "# Initialize the encoder and decoder.\n",
        "encoder = EncoderCNN(embed_size)\n",
        "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
        "\n",
        "# Move models to GPU if CUDA is available.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder.to(device)\n",
        "decoder.to(device)\n",
        "\n",
        "# Define the loss function.\n",
        "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
        "\n",
        "# Specify the learnable parameters of the model.\n",
        "params = list(decoder.parameters())+list(encoder.embed.parameters())\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = torch.optim.Adam(params,lr=0.001)\n",
        "\n",
        "# Set the total number of training steps per epoch.\n",
        "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
      ],
      "metadata": {
        "id": "_A7NfiahWA2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import os\n",
        "import requests\n",
        "import time\n",
        "\n",
        "# Open the training log file.\n",
        "f = open(log_file, 'w')\n",
        "\n",
        "old_time = time.time()\n",
        "response = requests.request(\"GET\",\n",
        "                            \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\",\n",
        "                            headers={\"Metadata-Flavor\":\"Google\"})\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "\n",
        "    for i_step in range(1, total_step+1):\n",
        "\n",
        "        if time.time() - old_time > 60:\n",
        "            old_time = time.time()\n",
        "            requests.request(\"POST\",\n",
        "                             \"https://nebula.udacity.com/api/v1/remote/keep-alive\",\n",
        "                             headers={'Authorization': \"STAR \" + response.text})\n",
        "\n",
        "        # Randomly sample a caption length, and sample indices with that length.\n",
        "        indices = data_loader.dataset.get_train_indices()\n",
        "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
        "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
        "        data_loader.batch_sampler.sampler = new_sampler\n",
        "\n",
        "        # Obtain the batch.\n",
        "        images, captions = next(iter(data_loader))\n",
        "\n",
        "        # Move batch of images and captions to GPU if CUDA is available.\n",
        "        images = images.to(device)\n",
        "        captions = captions.to(device)\n",
        "\n",
        "        # Zero the gradients.\n",
        "        decoder.zero_grad()\n",
        "        encoder.zero_grad()\n",
        "\n",
        "        # Pass the inputs through the CNN-RNN model.\n",
        "        features = encoder(images)\n",
        "        outputs = decoder(features, captions)\n",
        "\n",
        "        # Calculate the batch loss.\n",
        "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
        "\n",
        "        # Backward pass.\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the parameters in the optimizer.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Get training statistics.\n",
        "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
        "\n",
        "        # Print training statistics (on same line).\n",
        "        print('\\r' + stats, end=\"\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # Print training statistics to file.\n",
        "        f.write(stats + '\\n')\n",
        "        f.flush()\n",
        "\n",
        "        # Print training statistics (on different line).\n",
        "        if i_step % print_every == 0:\n",
        "            print('\\r' + stats)\n",
        "\n",
        "    # Save the weights.\n",
        "    if epoch % save_every == 0:\n",
        "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
        "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
        "\n",
        "# Close the training log file.\n",
        "f.close()"
      ],
      "metadata": {
        "id": "pmUFNPPIWAy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/opt/cocoapi/PythonAPI')\n",
        "from pycocotools.coco import COCO\n",
        "from data_loader import get_loader\n",
        "from torchvision import transforms\n",
        "\n",
        "# TODO #1: Define a transform to pre-process the testing images.\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
        "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
        "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
        "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
        "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
        "                         (0.229, 0.224, 0.225))])\n",
        "\n",
        "#-#-#-# Do NOT modify the code below this line. #-#-#-#\n",
        "\n",
        "# Create the data loader.\n",
        "data_loader = get_loader(transform=transform_test,\n",
        "                         mode='test')"
      ],
      "metadata": {
        "id": "isRmvZieXhSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Obtain sample image before and after pre-processing.\n",
        "orig_image, image = next(iter(data_loader))\n",
        "\n",
        "# Visualize sample image, before pre-processing.\n",
        "plt.imshow(np.squeeze(orig_image))\n",
        "plt.title('example image')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2VR2KsYXhPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "4Hjam9WcXhM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "j_t75d0mXhKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Watch for any changes in model.py, and re-load it automatically.\n",
        "% load_ext autoreload\n",
        "% autoreload 2\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from model import EncoderCNN, DecoderRNN\n",
        "\n",
        "# Specify the saved models to load.\n",
        "encoder_file = 'encoder-3.pkl'\n",
        "decoder_file = 'decoder-3.pkl'\n",
        "\n",
        "# Select appropriate values for the Python variables below.\n",
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "\n",
        "# The size of the vocabulary.\n",
        "vocab_size = len(data_loader.dataset.vocab)\n",
        "\n",
        "# Initialize the encoder and decoder, and set each to inference mode.\n",
        "encoder = EncoderCNN(embed_size)\n",
        "encoder.eval()\n",
        "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
        "decoder.eval()\n",
        "\n",
        "# Load the trained weights.\n",
        "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
        "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
        "\n",
        "# Move models to GPU if CUDA is available.\n",
        "encoder.to(device)\n",
        "decoder.to(device)"
      ],
      "metadata": {
        "id": "95rjpogyXhHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move image Pytorch Tensor to GPU if CUDA is available.\n",
        "image = image.to(device)\n",
        "\n",
        "# Obtain the embedded image features.\n",
        "features = encoder(image).unsqueeze(1)\n",
        "\n",
        "# Pass the embedded image features through the model to get a predicted caption.\n",
        "output = decoder.sample(features)\n",
        "print('example output:', output)\n",
        "\n",
        "assert (type(output)==list), \"Output needs to be a Python list\"\n",
        "assert all([type(x)==int for x in output]), \"Output should be a list of integers.\"\n",
        "assert all([x in data_loader.dataset.vocab.idx2word for x in output]), \"Each entry in the output needs to correspond to an integer that indicates a token in the vocabulary.\""
      ],
      "metadata": {
        "id": "ssTuYuaWXhEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO #4: Complete the function.\n",
        "def clean_sentence(output):\n",
        "\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "-_bxyFuKXhCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_sentence(output):\n",
        "    cleaned_list = []\n",
        "    for index in output:\n",
        "        if  (index == 1) :\n",
        "            continue\n",
        "        cleaned_list.append(data_loader.dataset.vocab.idx2word[index])\n",
        "    cleaned_list = cleaned_list[1:-1] # Discard <start> and <end>\n",
        "\n",
        "    sentence = ' '.join(cleaned_list) # Convert list of string to\n",
        "    sentence = sentence.capitalize()\n",
        "    return sentence\n"
      ],
      "metadata": {
        "id": "24CIJydhXhA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = clean_sentence(output)\n",
        "print('example sentence:', sentence)\n",
        "\n",
        "assert type(sentence)==str, 'Sentence needs to be a Python string!'"
      ],
      "metadata": {
        "id": "aiMmlk11Xx3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prediction():\n",
        "    orig_image, image = next(iter(data_loader))\n",
        "    plt.imshow(np.squeeze(orig_image))\n",
        "    plt.title('Sample Image')\n",
        "    plt.show()\n",
        "    image = image.to(device)\n",
        "    features = encoder(image).unsqueeze(1)\n",
        "    output = decoder.sample(features)\n",
        "    sentence = clean_sentence(output)\n",
        "    print(sentence)"
      ],
      "metadata": {
        "id": "7MEXojFuXx0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_prediction()"
      ],
      "metadata": {
        "id": "leMYYmr4XxyA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}